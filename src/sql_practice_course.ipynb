{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43b771d2",
   "metadata": {},
   "source": [
    "# SQL Practice Course: From Local to Production\n",
    "\n",
    "This notebook is designed to help you build SQL skills step by step.\n",
    "You'll start with a small, local SQLite database and progress to connecting\n",
    "to larger-scale relational databases like PostgreSQL, as well as cloud warehouses\n",
    "such as BigQuery and Snowflake.\n",
    "\n",
    "Throughout this notebook you'll find exercises that ask you to write SQL queries.\n",
    "You'll also see example Python code showing how to connect to each type of database\n",
    "from Python using appropriate libraries.\n",
    "\n",
    "> **Tip:** When running queries locally, use the `run_sql()` helper provided below.\n",
    "For other databases, follow the connection examples to create your own helper functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a114e935",
   "metadata": {},
   "source": [
    "## Helper for Local SQLite\n",
    "\n",
    "The following function opens a connection to the SQLite database that\n",
    "comes with this course. Use it to run your queries against the local database.\n",
    "\n",
    "The database file is located under `data/companycam_practice.db`. If you would\n",
    "like to explore the schema, you can run a query like `SELECT name FROM sqlite_master WHERE type='table';`\n",
    "to list all tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e43d9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the local SQLite database\n",
    "DB_PATH = \"data/companycam_practice.db\"\n",
    "\n",
    "def run_sql(query: str, params: dict | None = None) -> pd.DataFrame:\n",
    "    '''\n",
    "    Execute a SQL query against the local SQLite database and return a pandas DataFrame.\n",
    "    '''\n",
    "    with sqlite3.connect(DB_PATH) as conn:\n",
    "        return pd.read_sql_query(query, conn, params=params)\n",
    "\n",
    "# Example usage (uncomment to run):\n",
    "# df = run_sql(\"SELECT * FROM contractors LIMIT 5\")\n",
    "# df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0241114e",
   "metadata": {},
   "source": [
    "## Level 1: Local Databases (SQLite & DuckDB)\n",
    "\n",
    "In this section you’ll practice SQL on a small, local database. \n",
    "\n",
    "SQLite is a lightweight, file‑based database that doesn’t require a server. \n",
    "You can use it for quick experiments, unit tests, or simple applications.\n",
    "\n",
    "DuckDB is another local database you can use for analytical workloads. It \n",
    "reads column‑oriented data formats (like Parquet) directly and is great for working with\n",
    "large analytical datasets locally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab9143e",
   "metadata": {},
   "source": [
    "### Exercise 1: Contractors in Miami\n",
    "\n",
    "List all contractors who are located in the city of **Miami**. Your result\n",
    "should include all columns from the `contractors` table for these rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbc8351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write your SQL query below\n",
    "# Use run_sql(\"...\", params=None) to execute your query\n",
    "\n",
    "# Example:\n",
    "# df = run_sql(\"SELECT * FROM contractors WHERE city = 'Miami'\")\n",
    "# df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705209b7",
   "metadata": {},
   "source": [
    "*Hint:* Use `WHERE city = 'Miami'`.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a072b5",
   "metadata": {},
   "source": [
    "### Exercise 2: Most Recent Photos\n",
    "\n",
    "Find the **five most recent** photos that have been uploaded. The result\n",
    "should show the photo ID, job ID, contractor ID, and upload time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef259bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write your SQL query below\n",
    "# Example: order by the timestamp column descending and limit 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44249e35",
   "metadata": {},
   "source": [
    "*Hint:* Order by the upload timestamp descending and limit 5.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468ce061",
   "metadata": {},
   "source": [
    "### Exercise 3: Job Counts by Contractor\n",
    "\n",
    "Count how many jobs each contractor has. Return the contractor ID and the job count,\n",
    "sorted by the job count in descending order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3371d94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write your SQL query below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261a8b7e",
   "metadata": {},
   "source": [
    "*Hint:* Use `GROUP BY contractor_id` and `COUNT(*)`.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a9d94d",
   "metadata": {},
   "source": [
    "### Optional: Using DuckDB Locally\n",
    "\n",
    "If you’d like to explore DuckDB, here is how you can create an in‑memory \n",
    "connection in Python. DuckDB is similar to SQLite but designed for analytics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea3ffbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example DuckDB usage (uncomment to run if you have duckdb installed)\n",
    "# import duckdb\n",
    "# con = duckdb.connect()\n",
    "# result = con.sql(\"SELECT 42 AS answer\").fetchall()\n",
    "# result\n",
    "\n",
    "# To connect to a persistent file instead:\n",
    "# con = duckdb.connect('data/companycam_practice.db')\n",
    "# df = con.sql('SELECT COUNT(*) FROM photos').df()\n",
    "# df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c384329e",
   "metadata": {},
   "source": [
    "## Level 2: Production Databases (PostgreSQL)\n",
    "\n",
    "Moving from local experiments to production often means using a networked database. \n",
    "PostgreSQL is a powerful, open‑source relational database. \n",
    "Unlike SQLite, a PostgreSQL database runs as a service you connect to over a network.\n",
    "\n",
    "The typical way to interact with PostgreSQL from Python is via a library called\n",
    "**SQLAlchemy**, which provides a high‑level API for connecting, executing queries, and managing\n",
    "connections.\n",
    "\n",
    "Below is an example function that uses SQLAlchemy to connect to a PostgreSQL database.\n",
    "You’ll need to set your own connection string via an environment variable.\n",
    "Make sure you install the necessary driver (psycopg2) in your environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3844e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example helper for PostgreSQL using SQLAlchemy\n",
    "# from sqlalchemy import create_engine, text\n",
    "# import os, pandas as pd\n",
    "#\n",
    "# POSTGRES_URL = os.environ.get('POSTGRES_URL', 'postgresql+psycopg2://user:password@host:5432/dbname')\n",
    "# engine = create_engine(POSTGRES_URL, pool_pre_ping=True)\n",
    "#\n",
    "# def run_pg(sql: str, params: dict | None = None) -> pd.DataFrame:\n",
    "#     '''Run a SQL query against a PostgreSQL database using SQLAlchemy.'''\n",
    "#     with engine.connect() as conn:\n",
    "#         return pd.read_sql(text(sql), conn, params=params)\n",
    "#\n",
    "# # Example usage:\n",
    "# # df = run_pg(\"SELECT COUNT(*) FROM photos\")\n",
    "# # df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af69e16b",
   "metadata": {},
   "source": [
    "### Exercise 4: Join Jobs and Contractors (PostgreSQL)\n",
    "\n",
    "Assuming you have migrated the local tables to a PostgreSQL database, write a\n",
    "query to list all job sites together with the names of their assigned contractors.\n",
    "Sort the result by job ID.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615300e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: If you have a PostgreSQL database set up and accessible via run_pg, write your query below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26c230a",
   "metadata": {},
   "source": [
    "*Hint:* Perform an INNER JOIN between `jobs` and `contractors` on the contractor ID.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266d50d8",
   "metadata": {},
   "source": [
    "### Exercise 5: Contractors Without Photos\n",
    "\n",
    "Find the contractors who have **not uploaded any photos**. This requires joining the contractors table with the photos table and looking for contractors with no matching photos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8475b701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write your SQL query for PostgreSQL here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0feef8b",
   "metadata": {},
   "source": [
    "*Hint:* Use a LEFT JOIN between contractors and photos and filter for NULL photo IDs.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d3c786",
   "metadata": {},
   "source": [
    "## Level 3: Cloud Warehouses (BigQuery & Snowflake)\n",
    "\n",
    "For truly large datasets or highly concurrent analytics, companies often use cloud data warehouses.\n",
    "Two popular options are **BigQuery** (Google Cloud) and **Snowflake**. These services separate compute\n",
    "and storage, provide automatic scaling, and can handle petabytes of data.\n",
    "\n",
    "In Python, you access these warehouses via their official client libraries. Below\n",
    "are simple examples for connecting to BigQuery and Snowflake.\n",
    "\n",
    "Before running these examples, you need to install the respective SDKs and set\n",
    "up authentication using service accounts or environment variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03810bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example connection to BigQuery\n",
    "# from google.cloud import bigquery\n",
    "# from google.oauth2 import service_account\n",
    "# import os, pandas as pd\n",
    "#\n",
    "# # Path to your service account JSON key file\n",
    "# key_path = os.environ.get('GOOGLE_APPLICATION_CREDENTIALS', 'path/to/service-account.json')\n",
    "# credentials = service_account.Credentials.from_service_account_file(key_path)\n",
    "# project_id = os.environ.get('GCP_PROJECT_ID', 'your-project-id')\n",
    "# client = bigquery.Client(credentials=credentials, project=project_id)\n",
    "#\n",
    "# def run_bq(sql: str) -> pd.DataFrame:\n",
    "#     '''Run a SQL query against BigQuery and return a DataFrame.'''\n",
    "#     query_job = client.query(sql)\n",
    "#     return query_job.to_dataframe()\n",
    "#\n",
    "# # Example usage:\n",
    "# # df = run_bq(\"SELECT COUNT(*) AS photo_count FROM `dataset.photos`\")\n",
    "# # df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3f2ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example connection to Snowflake\n",
    "# import snowflake.connector\n",
    "# import pandas as pd\n",
    "# import os\n",
    "#\n",
    "# conn = snowflake.connector.connect(\n",
    "#     user=os.environ.get('SNOWFLAKE_USER'),\n",
    "#     password=os.environ.get('SNOWFLAKE_PASSWORD'),\n",
    "#     account=os.environ.get('SNOWFLAKE_ACCOUNT'),\n",
    "#     warehouse=os.environ.get('SNOWFLAKE_WAREHOUSE'),\n",
    "#     database=os.environ.get('SNOWFLAKE_DATABASE'),\n",
    "#     schema=os.environ.get('SNOWFLAKE_SCHEMA')\n",
    "# )\n",
    "#\n",
    "# def run_sf(sql: str) -> pd.DataFrame:\n",
    "#     '''Run a SQL query against Snowflake and return a DataFrame.'''\n",
    "#     return pd.read_sql(sql, conn)\n",
    "#\n",
    "# # Example usage:\n",
    "# # df = run_sf(\"SELECT COUNT(*) FROM photos\")\n",
    "# # df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7049ad65",
   "metadata": {},
   "source": [
    "### Exercise 6: Average Photo Size (BigQuery)\n",
    "\n",
    "Using BigQuery (or Snowflake), calculate the **average photo file size** per job.\n",
    "Return the job ID and the average size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f0bef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write your SQL query for BigQuery or Snowflake here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3431d7c9",
   "metadata": {},
   "source": [
    "*Hint:* Aggregate by `job_id` and use `AVG(size_kb)`.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9ac3db",
   "metadata": {},
   "source": [
    "### Exercise 7: Recently Active Contractors (Cloud)\n",
    "\n",
    "Label contractors as **Active** if they uploaded a photo in the last 30 days, and **Dormant** otherwise.\n",
    "This exercise is often used for feature engineering in machine learning pipelines.\n",
    "\n",
    "Write a SQL query in BigQuery or Snowflake that returns the contractor ID, last upload date,\n",
    "and the label (Active or Dormant).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690b52ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write your SQL query here for BigQuery or Snowflake\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa321e19",
   "metadata": {},
   "source": [
    "*Hint:* Compare the most recent upload timestamp per contractor to the current date minus 30 days.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afbd0ec",
   "metadata": {},
   "source": [
    "## Production Considerations\n",
    "\n",
    "Connecting to databases in production involves more than writing a SQL query. Here are some considerations:\n",
    "\n",
    "- **Configuration and Secrets:** Store database credentials and project IDs in environment variables or secret managers.  Do not hard‑code usernames or passwords in your code.\n",
    "- **Connection Pooling:** Use a connection pool (e.g., via SQLAlchemy) to reuse connections efficiently and avoid exhausting database resources.\n",
    "- **Parameterization:** Always parameterize your queries rather than using string concatenation. This guards against SQL injection and improves query planning.\n",
    "- **Chunking and Streaming:** When processing large result sets, iterate in chunks (e.g., using `chunksize` in pandas) instead of loading everything into memory at once.\n",
    "- **Indexing and Query Plans:** Add indexes on columns that are frequently joined or filtered. Use your database’s EXPLAIN functionality to understand how queries are executed.\n",
    "- **Cost Control (Cloud Warehouses):** Cloud warehouses charge based on data scanned and compute usage. Limit the columns selected, use partition pruning, and schedule jobs during off‑peak hours to reduce cost.\n",
    "- **Testing and Migrations:** Use migration tools (like Alembic for PostgreSQL) and testing frameworks (like dbt tests) to ensure your schema and data meet quality expectations.\n",
    "\n",
    "By following these practices, you can write queries that are not only correct but also efficient, secure, and production‑ready.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
